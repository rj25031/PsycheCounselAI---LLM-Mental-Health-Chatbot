{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install transformers  wandb vllm\n"
      ],
      "metadata": {
        "id": "aGhJAfUdyUWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK3spx44yPQI",
        "outputId": "c7c73c12-0326-4814-9d7c-3222e0b454bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "lora_rank = 32\n",
        "dtype = None\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"rj25031/psyco-counsil-ai\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    fast_inference = True,\n",
        "    max_lora_rank = lora_rank,\n",
        "    dtype = dtype,\n",
        "    gpu_memory_utilization = 0.6,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok transformers torch nest_asyncio pymongo sentence-transformers scikit-learn -q\n",
        "\n",
        "!ngrok config add-authtoken your_ngrock_api_key\n"
      ],
      "metadata": {
        "id": "BRmmtC538DqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import re\n",
        "import nest_asyncio\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "app.prompt_template = \"\"\"You are a licensed mental health therapist conducting a private counseling session. Your tone should be warm, empathetic, and non-judgmental. Provide thoughtful, supportive responses based on the clientâ€™s concerns.\n",
        "\n",
        "### Current Client Message:\n",
        "{user_input}\n",
        "\n",
        "### Your Response as the Therapist:\n",
        "\"\"\"\n",
        "\n",
        "class ChatInput(BaseModel):\n",
        "    prompt: str\n",
        "\n",
        "def generate_response(user_input: str):\n",
        "    full_prompt = app.prompt_template.format(user_input=user_input)\n",
        "    inputs = tokenizer([full_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150\n",
        "    )\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    match = re.search(r\"### Your Response as the Therapist:\\n(.*?)(?:###|$)\", response, re.DOTALL)\n",
        "    return match.group(1).strip() if match else \"I'm here to support youâ€”could you tell me more?\"\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat(data: ChatInput):\n",
        "    response = generate_response(data.prompt)\n",
        "    return {\"response\": response}\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f\"Public API URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=False)\n"
      ],
      "metadata": {
        "id": "m9iKgdcCxl-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}